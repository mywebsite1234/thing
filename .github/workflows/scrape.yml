# This is a GitHub Actions workflow file.
# It defines an automated process that runs on GitHub's servers.

name: Website Scraper

# This allows you to run the workflow manually from the Actions tab
on:
  workflow_dispatch:
    inputs:
      url:
        description: 'The full URL of the website to scrape'
        required: true

jobs:
  scrape-and-package:
    # The job will run on a standard Ubuntu virtual machine provided by GitHub
    runs-on: ubuntu-latest

    steps:
      # Step 1: A standard checkout step (required for workflows)
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Set up Node.js on the virtual machine
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      # Step 3: Install the necessary libraries (axios for downloading, cheerio for parsing)
      - name: Install dependencies
        run: npm install axios cheerio

      # Step 4: Run the website extractor script
      - name: Run Website Extractor Script
        id: scrape_script
        run: |
          # Set the domain name as an output for the next step
          DOMAIN=$(echo "${{ github.event.inputs.url }}" | sed -e 's|https://||' -e 's|http://||' -e 's|www.||' | cut -d/ -f1)
          echo "domain_name=$DOMAIN" >> $GITHUB_OUTPUT

          node -e "
            const axios = require('axios');
            const cheerio = require('cheerio');
            const fs = require('fs');
            const path = require('path');
            const url = require('url');

            const baseUrl = '${{ github.event.inputs.url }}';
            const parsedBaseUrl = new url.URL(baseUrl);
            const domainName = '$DOMAIN'; // Use the domain name calculated above

            console.log(\`[*] Starting scrape for: \${baseUrl}\`);
            console.log(\`[*] Files will be saved in the '\${domainName}' directory.\`);

            async function downloadAsset(assetUrl, savePath) {
              try {
                const dir = path.dirname(savePath);
                if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });

                const response = await axios({ method: 'GET', url: assetUrl, responseType: 'stream' });
                const writer = fs.createWriteStream(savePath);
                response.data.pipe(writer);

                return new Promise((resolve, reject) => {
                  writer.on('finish', resolve);
                  writer.on('error', reject);
                });
              } catch (error) {
                console.error(\`  [!] Failed to download \${assetUrl}: \${error.message}\`);
              }
            }

            async function scrape() {
              try {
                const response = await axios.get(baseUrl);
                const \$ = cheerio.load(response.data);
                const assetPromises = [];
                const downloadedUrls = new Set();

                $('link[href], script[src], img[src], source[src]').each((i, element) => {
                  const assetAttr = \$(element).is('link') ? 'href' : 'src';
                  const assetUrl = \$(element).attr(assetAttr);
                  if (!assetUrl) return;

                  const absoluteUrl = new url.URL(assetUrl, baseUrl).href;
                  if (new url.URL(absoluteUrl).hostname === parsedBaseUrl.hostname && !downloadedUrls.has(absoluteUrl)) {
                    downloadedUrls.add(absoluteUrl);
                    const assetPath = new url.URL(absoluteUrl).pathname;
                    let localPath = path.join(domainName, assetPath.startsWith('/') ? assetPath.substring(1) : assetPath);
                    if (localPath.endsWith('/') || path.extname(localPath) === '') localPath = path.join(localPath, 'index.html');
                    
                    console.log(\`  [+] Queued: \${absoluteUrl}\`);
                    assetPromises.push(downloadAsset(absoluteUrl, localPath));
                  }
                });

                fs.writeFileSync(path.join(domainName, 'index.html'), response.data);
                console.log('[*] Saved main index.html page.');
                
                await Promise.all(assetPromises);
                console.log('\n[*] Asset download complete!');
              } catch (error) {
                console.error(\`[!] CRITICAL ERROR: \${error.message}\`);
                process.exit(1);
              }
            }

            scrape();
          "
      # Step 5: Take the folder with the downloaded website and upload it as an artifact
      - name: Upload artifact
        uses: actions/upload-artifact@v4 # <-- THIS IS THE FIX
        with:
          name: website-files
          path: ${{ steps.scrape_script.outputs.domain_name }}

