# This is a GitHub Actions workflow file.
# It defines an automated process that runs on GitHub's servers.

name: Website Scraper

# This allows you to run the workflow manually from the Actions tab
on:
  workflow_dispatch:
    inputs:
      url:
        description: 'The full URL of the website to scrape'
        required: true

jobs:
  scrape-and-package:
    # The job will run on a standard Ubuntu virtual machine provided by GitHub
    runs-on: ubuntu-latest

    steps:
      # Step 1: A standard checkout step (required for workflows)
      - name: Checkout repository
        uses: actions/checkout@v3

      # Step 2: Set up Node.js on the virtual machine
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      # Step 3: Install the necessary libraries (axios for downloading, cheerio for parsing)
      - name: Install dependencies
        run: npm install axios cheerio

      # Step 4: Run the website extractor script
      # The Node.js script is embedded directly here for simplicity.
      - name: Run Website Extractor Script
        run: |
          node -e "
            const axios = require('axios');
            const cheerio = require('cheerio');
            const fs = require('fs');
            const path = require('path');
            const url = require('url');

            // The URL is passed in from the manual trigger on GitHub
            const baseUrl = '${{ github.event.inputs.url }}';
            const parsedBaseUrl = new url.URL(baseUrl);
            const domainName = parsedBaseUrl.hostname.replace('www.', '');

            console.log(`[*] Starting scrape for: \${baseUrl}`);
            console.log(`[*] Files will be saved in the '\${domainName}' directory.`);

            async function downloadAsset(assetUrl, savePath) {
              try {
                const dir = path.dirname(savePath);
                if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });

                const response = await axios({ method: 'GET', url: assetUrl, responseType: 'stream' });
                const writer = fs.createWriteStream(savePath);
                response.data.pipe(writer);

                return new Promise((resolve, reject) => {
                  writer.on('finish', resolve);
                  writer.on('error', reject);
                });
              } catch (error) {
                console.error(`  [!] Failed to download \${assetUrl}: \${error.message}`);
              }
            }

            async function scrape() {
              try {
                const response = await axios.get(baseUrl);
                const \$ = cheerio.load(response.data);
                const assetPromises = [];
                const downloadedUrls = new Set();

                $('link[href], script[src], img[src], source[src]').each((i, element) => {
                  const assetAttr = \$(element).is('link') ? 'href' : 'src';
                  const assetUrl = \$(element).attr(assetAttr);
                  if (!assetUrl) return;

                  const absoluteUrl = new url.URL(assetUrl, baseUrl).href;
                  if (new url.URL(absoluteUrl).hostname === parsedBaseUrl.hostname && !downloadedUrls.has(absoluteUrl)) {
                    downloadedUrls.add(absoluteUrl);
                    const assetPath = new url.URL(absoluteUrl).pathname;
                    let localPath = path.join(domainName, assetPath.startsWith('/') ? assetPath.substring(1) : assetPath);
                    if (localPath.endsWith('/')) localPath = path.join(localPath, 'index.html');
                    
                    console.log(`  [+] Queued: \${absoluteUrl}`);
                    assetPromises.push(downloadAsset(absoluteUrl, localPath));
                  }
                });

                fs.writeFileSync(path.join(domainName, 'index.html'), response.data);
                console.log('[*] Saved main index.html page.');
                
                await Promise.all(assetPromises);
                console.log('\n[*] Asset download complete!');
              } catch (error) {
                console.error(`[!] CRITICAL ERROR: \${error.message}`);
                process.exit(1);
              }
            }

            scrape();
          "
      # Step 5: Take the folder with the downloaded website and upload it as an artifact
      - name: Upload artifact
        uses: actions/upload-artifact@v3
        with:
          name: website-files
          # The path is the folder named after the website's domain
          path: |
            ${{ env.DOMAIN_NAME }}
        env:
          DOMAIN_NAME: ${{ fromJson('{"url": "${{ github.event.inputs.url }}"}').url.replace('https://', '').replace('http://', '').split('/')[0].replace('www.','') }}
