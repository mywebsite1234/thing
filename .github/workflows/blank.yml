# This is a GitHub Actions workflow file.
# It defines an automated process that runs on GitHub's servers.

name: Website Scraper

# This allows you to run the workflow manually from the Actions tab
on:
  workflow_dispatch:
    inputs:
      url:
        description: 'The full URL of the website to scrape'
        required: true

jobs:
  scrape-and-package:
    # The job will run on a standard Ubuntu virtual machine provided by GitHub
    runs-on: ubuntu-latest

    steps:
      # Step 1: A standard checkout step (required for workflows)
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Set up Node.js on the virtual machine
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      # Step 3: Install Puppeteer, a tool for controlling a real browser
      - name: Install dependencies
        run: npm install puppeteer

      # Step 4: Run the website extractor script using Puppeteer
      - name: Run Website Extractor Script
        id: scrape_script
        run: |
          # Set the domain name as an output for the next step
          DOMAIN=$(echo "${{ github.event.inputs.url }}" | sed -e 's|https://||' -e 's|http://||' -e 's|www.||' | cut -d/ -f1)
          echo "domain_name=$DOMAIN" >> $GITHUB_OUTPUT

          # Create the directory for the output
          mkdir -p $DOMAIN

          node -e "
            const puppeteer = require('puppeteer');
            const fs = require('fs');
            const path = require('path');

            const baseUrl = '${{ github.event.inputs.url }}';
            const domainName = '$DOMAIN';
            const savePath = path.join(domainName, 'index.html');

            console.log(\`[*] Starting scrape for: \${baseUrl}\`);

            (async () => {
              try {
                console.log('[*] Launching headless browser...');
                // Puppeteer needs --no-sandbox flag to run in GitHub Actions environment
                const browser = await puppeteer.launch({ args: ['--no-sandbox', '--disable-setuid-sandbox'] });
                const page = await browser.newPage();
                
                console.log(\`[*] Navigating to \${baseUrl}...\`);
                // Increased timeout for complex pages
                await page.goto(baseUrl, { waitUntil: 'networkidle2', timeout: 90000 });
                
                console.log('[*] Page loaded. Getting content...');
                const content = await page.content();
                
                fs.writeFileSync(savePath, content);
                console.log(\`[+] Successfully saved page to: \${savePath}\`);

                await browser.close();
                console.log('[*] Browser closed. Scrape complete!');

              } catch (error) {
                console.error(\`[!] CRITICAL ERROR: \${error.message}\`);
                process.exit(1);
              }
            })();
          "
      # Step 5: Take the folder with the downloaded website and upload it as an artifact
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: website-files
          path: ${{ steps.scrape_script.outputs.domain_name }}

